import urllib.request

response = urllib.request.urlopen('http://www.baidu.com')

print(response)

rawHtml = response.read().decode('utf-8')

print(rawHtml)

# getHtmld的function
--------------------------------------------------------------------
def getHtml():
    URL = 'https://www.douban.com/'
    header = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.71 Safari/537.36'
    }
    request = urllib.request.Request(url=URL,headers = header)

    html = ''

    try:
        response = urllib.request.urlopen(request)
        html = response.read().decode('utf-8')
    
    except urllib.error.URLError as e:
        if hasattr(e,'code'):
            print(e.code)
        
        if hasattr(e,'reason'):
            print(e.reason)

    return html

---------------------------------------------------------------------------------
    
#安装beautiful soup 的库

pip install beautifulsoup4

#安装解析库

pip install lxml

pip install html5lib

from bs4 import BeautifulSoup
soup = BeautifulSoup(html,features='lxml')

-----------------------------------------------------------------------------------

#Beautiful Soup选择器用来查找定位元素，并获取数据


# 1.节点选择器，2方法选择器，3 CSS选择器

#节点就是html中的tag多个节点只会提取其中的一个

#soup.tag比如soup.title, soup.a

#提取具体信息可以根据tag 里的参数名来提取
#用来提起节点名称
soup.tag.name
#用来提取节点属性
soup.tag.attrs
#用来提取内容
soup.tag.strings

-------------------------------------------------------------------------------------
#关联选择器
#子节点 contents,children
#子孙节点 descendants
#父节点 parent
# 祖先节点 parents
# 兄弟节点 next_sblings, previous_sblings,previous_sbling

#soup.ul.contents能得到ul下的所有li值，返回的是一个列表
liList = soup.ul.contents
print(liList)

#如果用的是children返回的就是一个生成器
liIterator = soup.ul.children
for i,c in enumerate(liIterator):
    print(c)

#next_sblings返回的同样是个生成器，next_sibling不好用
print(list(enumerate(soup.li.next_siblings)))

-----------------------------------------------------------------------------------------
css选择器

print(soup.select('#anony-nav'))#id 定位

print(soup.select('.anony-nav-links')) # class 定位

print(soup.select('a')) 标签选择器这样会获得所有的a

print(soup.select('#anony-nav .anony-nav-links ul li'))#混合选择器

#用select选择器得到的还是一个soup element

#所以能用soup.tag.attrs

---------------------------------------------------------------------------------

#方法选择器
#find_all(name,attrs,recursive,text,**kwargs)

print(soup.find_all('a'))

soup.find_all(re.compile('^b') #查找所有以b开头的标签

soup.find_all(['a','span'])#查找所有a标签和span标签

#attrs参数接受的字典
soup.find_all(attrs={id:link1})

#kwargs参数接受变量赋值
soup.find_all(id='link1')

#因为class是python的关键字所以更具class查询
print(soup.find_all(class_ = 'story'))

#text参数是通过标签中的内容来解锁的
print(soup.find_all(text='Elise'))
print(soup.find_all('a',text='Elise'))

#limit参数，用于限制返回结果参数
比如print(soup.find_all('a',limit=2))

#recursive参数，作用决定是否获取子孙节点
print(soup.find_all('title',recursive=False))




